\section{Methods} \label{sec:methods}

This section details the algorithm and implementation design used in this work.
After briefly recounting the core mechanics of hereditary stratigraphy methods for phylogenetic tracking, we motivate and describe new lightweight ``surface'' data structures used to annotate agent genomes in this work.
Then, we describe the asynchronous compute-communicate implementation strategy behind the testbed WSE-oriented island-based genetic algorithm used for validation and benchmarking experiments.

\input{fig/schematic}

\subsection{Distributed Phylogenetic Tracking}

Our approach to analysis of evolutionary history for wafer-scale simulation draws inspiration from the inference-based paradigm used to characterize history in natural systems.
This approach is fully-distributed, with ancestry information contained within genomes themselves rather than in any external tracking mechanism.
Just as mutational drift encodes ancestry information in DNA genomes, the inference-based paradigm requires only local updates to individual genomes at runtime as generations elapse.

Recent work introducing \textit{hereditary stratigraphy} (HStrat) methodology has explored how best to organize genetic material to maximize reconstruction quality and minimize memory footprint \citep{moreno2022hstrat, moreno2022hereditary}.
Proposed work bundles HStrat material with underlying agent genomes as an instrumentative ornament akin to non-coding DNA, entirely neutral with respect to agent traits or fitness.
Agents' \textit{HStrat} annotations can then be used to estimate their phylogenetic history after-the-fact, as depicted in Figure \ref{fig:runtime-posthoc-schematic}.

The hereditary stratigraphy algorithm associates each generation along individual lineages with an identifying ``fingerprint'' value, referred to as a differentia.
Each time a generation elapses, all offspring generate a new identifier value.
Within each offspring, generated identifiers append to an inherited running chronological record of differentia values.
To prevent $\mathcal{O}(n)$ space complexity as generations elapse, not all fingerprints can be kept.
Instead, excess fingerprints are pruned away according to a deterministic programme that ensures retention of differentia values from checkpoint generations spaced across evolutionary history.

Reducing differentia size also constitutes another important way to save memory space.
For instance, using a single-bit differentia would accrue a 32-fold savings over a one-word differentia and an 8-fold savings over a byte.
Although shrinking differentia size increases the probability of spurious collisions that result in overestimation of phylogenetic relatedness, this can be balanced by increasing the density with which differentiaare retained.
We anticipate that most scenarios will call for differentia sized on the order of a single bit or a byte.
However, benefits of small differentia cannot be accrued if differentia timepoints need to be stored.
In fact, storing one 32 or 64bit timestamp per differentia would likely bloat the majority of memory use for annotations, significantly reducing the amount of meaningful lineage-identifying differentia that can be stored.
Thus, existing algorithms provide calculations capabable of predicting the set of differentia that will be retained at any single point in time, allowing for the time stamps of differentia to be deduced solely from the current time point and the position of the site within the annotation.

Single-bit checkpoints have been shown to produce good quality phylogenies using only 96 bits per genome \citep{moreno2023toward}.
The semantic structure of HStrat annotations streamlines \textit{post hoc} phylogenetic reconstruction, which essentially boils down to a simple trie-building procedure
\citep{moreno2024analysis}.

Another notable complication in design of hereditary stratigraphy algorithms is the difference between ``steady'' and ``tilted'' retention patterns \citep{TODOFROMPREPRINT}.
In prior work, these are termed depth-proportional resolution and recency-proportional resolution.
This refers to the difffference in how retained time points are distributed over historical time.
For steady,retention, sampled time points are distributed evenly over history.
For tilted retention, more-recent time points are preferred.
So, there is tighter spacing between more recent time points and looser spacing between more ancient time points.
Intuitively, this reflects what is often the case in natural history work wehere we have higher absolute precision in pinpointing more recent events and looser absolute precision in pinpointing more ancient events.
Comparisons of reconstruction quality have shown that tilted algorithms give higher quality reconstructions for the same amount of space.
However, this is not always the case and new work (cite other paper) suggests that a hybrid approach where half of annotation space is allocated for each strategy may be robust and perform well across scenarios.
Figure \ref{fig:surf-algorithms} includes retention drip plots that illustrate the difference between steady and tilted algorithms.

\subsection{New Surface Algorithms}

Although designed with efficiency in mind, existing ``column''-based hereditary stratigraphy algorithms have several properties that make them a poor fit with a resource-constrained environment like the the Cerebras WSE:
\begin{enumerate}
\item an annotation size cap is guaranteed, but the entire space is not guaranteed to be used leading to a percentage of wasted space for strictly fixed-sized annotations,
\item current algorithm implementations use complex data structures with dynamically allocated memory to perform operations like set subtraction,
\item computation of retention policies can take time proportional to annotation size, with operations to maintain differentia in sorted order within the annotation also scaling linearly with annotation size, and
\item algorithms make use of divide and modulo operations, which are slow.
\end{enumerate}

\input{fig/surf-vs-column-schematic}

Thus, the objectives of this project required significant elaboration of hereditary stratigraphy algorithms.
We use a constant-time indexing scheme that maps each lineage's fingerprint stream onto a fixed-width memory buffer such that eviction of existing fingerprint values by new placements maintains a temporally-representative sampling over elapsed time .
We call this approach ``surface''-based to distinguish it from earlier hereditary stratigraphy methods.
Time stamps can be positionally inferred and thus do not need to be stored --- a several-fold savings that enables fingerprint values to be shrunk down and packed together.
Figure \ref{fig:surf-vs=column-schematic} illustrates the difference between the existing ``column'' based approach and the new ``surface'' based approach which streamlines the annotation generational update procedure to a single operation.

\input{fig/surf-algorithms}

Figure \ref{fig:surf-algorithms} depicts how our algorithms sequence depositions onto the surface (heatmap) and the resulting stratum retention patterns (drip plot).
We leave formal descriptions of the site selection algorithms developed for tilted and steady surface algorithms --- and the reverse procedure necessary to efficiently infer the deposition times of each site on a surface at an arbitrary timepoint in the surface to future work.
However, reference implementations of the described algorithms in Python are provided in supplementary material Listing TODO.
For this work, we also translated the tilted algorithm to Zig--like Cerebras Software Language for use on the WSE and, as an intermediate step, to the general-purpose Zig programming language.
These implementations can be found in associated software repositories with the project.

Notably, these algorithms are solving the more general problem of stream curation in a new and highly efficient way, and may find other uses in data stream applications \citep{TODOCITEPREPRINT}.

\subsection{Asynchronous Island-model Evolutionary Computation}

We apply an island-model genetic algorithm to instantiate an evolutionary process spanning PEs.
Island-model approaches are common in applications of parallel and distributed computing to evolutionary computation \citep{bennett1999building}.
Under this model, each processor element (PE) hosts an independent population.
Migration between PEs stitches island populations together into a common gene pool.

Core kernel activity proceeds via an update cycle performed on each PE, which comprises several steps.
Figure \ref{fig:async-ga-schematic} provides a schematic overview.

The first step of this update loop is to handle migration, depicted as blue-red arrows in Figure \ref{fig:async-ga-schematic}.
We adopt a fully asynchronous approach to migration between neighboring PEs.
Evolutionary processes tend to occur in asynchronous manner with arbitrary factors influencing things, so this is a reasonable relaxation to make.

Each PE maintains independent immigration buffers and emigration buffers dedicated to each cardinal neighbor, depicted in blue and red, respectively, in Figure \ref{fig:async-ga-schematic}.
On simulation startup, an asynchronous DSD receive operation is opened to accept genomes from neighboring PEs into the appropriate immigration buffer.
At startup, additionally, the emigration buffer is populated with one or more genomes copied the population.
Then, an asynchronous send request is opened to dispatch wavelets containing genome data from the emigration buffer to the neighbor.
Both operations register an on-completion callback to set a ``send complete'' or ``receive complete'' flag variable associated with their corresponding buffer.

Each update cycle, the main update loop tests all ``send complete'' and ``receive complete'' flags.
For each immigration flag that is set, corresponding received genomes are written into the main population buffer, replacing randomly-chosen population members.
Then, the flag is reset a new receive request is initiated.
Likewise, for each emigration flag set, corresponding send buffers are re-populated with randomly sampled genomes from the main population buffer.
Corresponding flags are then reset and new send requests initiated.
The bottom right corner of Figure \ref{fig:async-ga-schematic} summarizes this process.

The remainder of the main update loop handles evolutionary operations within the scope of the executing PE.
Each genome within the population is evaluated to produce a floating point fitness value.
Modular implementation ensures evaluation criteria can be chosen appropriate for underlying experimental objectives.
For the purposes this project, we will use a trivial fitness function that explicitly models additive accumulation of beneficial/deleterious mutations as a floating point value within each genome.

After evaluation, tournament selection is applied.
Each slot in the next generation is populated with a genome exhibiting maximal fitness among $n$ randomly sampled individuals, ties broken randomly.

Finally, a mutational operator is applied across all genomes in the next population.
As with evaluation criteria, modular implementation allows mutation operations to be defined based on experimental objectives.
Here, we use a simple Gaussian mutation on each genome's stored fitness value.
At this point, hereditary stratigraphy annotations --- discussed next --- are updated to reflect an elapsed generation.

The process then repeats, with self-activating wavelet dispatched to execute the next cycle of the main update loop.

\input{fig/genome-layout}

Kernel source code implementing described procedures can be viewed at \url{https://hopth.ru/cl}.
Our implementation is defined modularity with respect to genome size, layout, mutational operator, and fitness evaluation criteria, allowing for direct re-use of produced software for follow-on evolution experiments on WSE hardware.
Figure \ref{fig:genome-layout} shows an example genome layout including hstrat instrumentation.


In preparation for proposed work, we joined the Cerebras SDK program \citep{selig2022cerebras} and assembled Cerebras Software Language (CSL) software implementations that will be necessary to conduct experiments with WSE hardware.
For the time being, we have used Cerebras' hardware emulator to test our software implementations.
This section reports a small-scale experiment performed to validate the core software functionality.
In addition to this integration test, software quality has also been verified through an associated unit test suite.

Genomes were fixed-length 3 word arrays represented using data type \texttt{u32}.
Figure \ref{fig:validation-example:genomes} details the content and layout of genomes, and provides example genome values yielded from simulation.
Notably, the first sixteen bits were used to tag clade geneses.
At the outset of simulation, founding population members were each assigned a randomized tag value.
This value was inherited without mutation over the course of simulation.
Thus, it can be used to identify end-state genomes that evolved from the same original ancestor.

The send buffers were sized to hold one genome and the receive buffers to hold 4 genomes.

% A key question will be the extent to which intentionally desynchronizing affects performance, stability, and computation quality.

\subsection{Software and Data Availability}

Software, configuration files, and executable notebooks for this work are available at \url{https://github.com/mmore500/hstrat-wafer-scale} TODO.
CSL Software can be found on at \url{https://github.com/mmore500/wse-sketches/tree/v0.1.0} \citep{moreno2024wse}.
Data and supplemental materials are available via the Open Science Framework \url{https://osf.io/bfm2z/} \citep{foster2017open}.

All hereditary stratigraph annotation, reference phylogeny generation, and phylogenetic reconstruction tools used in this work are published in the \texttt{hstrat} Python package \citep{moreno2022hstrat}.
This project uses data formats and tools associated with the ALife Data Standards project \citep{lalejini2019data} and benefited from many pieces of open-source scientific software \citep{sand2014tqdist,2020SciPy-NMeth,harris2020array,reback2020pandas,mckinney-proc-scipy-2010,sukumaran2010dendropy,cock2009biopython,dolson2024phylotrackpy,torchiano2016effsize,waskom2021seaborn,hunter2007matplotlib,moreno2024apc,moreno2023teeplot,torchiano2016effsize,moreno2024pecking,moreno2024joinem,moreno2024hsurf}.

The experiments in this use the Cerebras SDK to compile and test on a hardware simulator \citep{TODOCITE}.
Access to the SDK can be requested, currently free of charge, via their website.
