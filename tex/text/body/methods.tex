\section{Methods} \label{sec:methods}

This section details the algorithm and implementation design used in this work.
After briefly recounting the core mechanics of hereditary stratigraphy methods for phylogenetic tracking, we motivate and describe new lightweight ``surface'' data structures used to annotate agent genomes in this work.
Then, we describe the asynchronous compute-communicate implementation strategy behind the testbed WSE-oriented island-based genetic algorithm used for validation and benchmarking experiments.

\input{fig/schematic}

\subsection{Distributed Phylogenetic Tracking}

Natural history of biological life operates under no extrinsic provision for interpretable record-keeping, yet efforts to study it have proved immensely fruitful.
This fact bodes well that scaled-up simulation studies can succeed a platform for rich hypothesis-driven experiments despite potential sacrifices to aspects of observability currently enjoyed with centralized, storage-rich processing.
Indeed, observational and analytical strategies already developed to confront limitations in biological data can solve, or at least guide, work with massively distributed evolution simulations.

In biology, decoupled processes of mutational drift encode ancestry information in DNA genomes.
Proposed methods approach analysis of evolutionary operates analogously, with ancestry information contained within genomes themselves rather than in any external tracking mechanism.
Phylogenetic history can then be estimated after the fact, as depicted in Figure \ref{fig:runtime-posthoc-schematic}.
This ensures low runtime communication overhead, as well as near-total resiliency to germane modes of data loss (e.g., dropped messages, hardware crash-out).

Recent work introducing \textit{hereditary stratigraphy} (hstrat) methodology has explored how best to organize genetic material to maximize reconstruction quality and minimize memory footprint \citep{moreno2022hstrat, moreno2022hereditary}.
hstrat material can be bundled to agent genomes in a manner akin to non-coding DNA, entirely neutral with respect to agent traits and fitness.

The hereditary stratigraphy algorithm associates each generation along individual lineages with an identifying ``fingerprint'' value, referred to as a differentia.
On birth, offspring generate a new differentia value and append it to an inherited chronological record of differentia values, each corresponding to a generation along that lineage.
Under this scheme, mismatching differentia can be used to delimit the extent of common ancestry.
This semantic streamlines \textit{post hoc} phylogenetic reconstruction to a simple trie-building procedure
\citep{moreno2024analysis}.

To save space, differentia may be pruned away.
However, care must be taken to ensure retention of checkpoint generations that maximize coverage across evolutionary history.
Downsizing bits per differentia can also save memory space.
For instance, single-bit differentiae occupy $32\times$ less memory than full-word differentia.
Such space savings can be invested to increase the quantity of differentia retained, improving the density of records' coverage across elapsed generations.
However, it comes at the cost of increased probability for spurious differentia value collisions, which can make two lineages appear more closely related than they actually are.
We anticipate that most use cases will call for differentia sized on the order of a single bit or a byte.
Indeed, single-bit differentiae have been shown to yield good quality phylogenies using only 96 bits per genome \citep{TODOOTHERPAPER}.

Small differentia size intensifies need for a lean data structure to back differentia record management.
In particular, shrinking differentia to a single bit absurdifies accompanying each with a generational timestamp.
To safely drop timestamps, though, we must have means to recalculate them on demand.
As such, all described algorithms include means to deduce timestamps of retained differentia solely from their storage index and the count of the record's elapsed generations.

As a final elaboration, design of hstrat annotations must also consider how available storage space should be allocated across the span of history.
In one possible strategy, retained time points would be distributed evenly across history.
In another, more recent time points would be preferred.
We term these as ``steady'' and ``tilted'' policies, respectively \citep{THEPAPERWHERETHETERMSAREFROMTODO}.
Note that prior hereditary stratigraphy work refers to them instead as ``depth-proportional'' and ``recency-proportional resolution.''
Comparisons of reconstruction quality have shown that tilted policy gives higher quality reconstructions from the same amount of reconstruction space in most --- but not all --- circumstances \citep{oTHERPAPERTODO}.
This pattern follows an intuition to prefer high absolute precision is more useful to resolving recent events than more ancient ones.
In practice, it may be desirable to use a hybrid approach that allocates half of available annotation space to each strategy \citep{oTHERPAPERTODO}.
Figure \ref{fig:surf-algorithms} includes retention drip plots that contrast behavior of steady and tilted algorithms.

\subsection{Surface-based Hereditary Stratigraphy Algorithms}

% Although designed with efficiency in mind, existing ``column''-based  have several properties that make them a poor fit with a resource-constrained environment like the the Cerebras WSE:
At the outset of this project, several problematic aspects of porting existing hereditary stratigraphy algorithms to the WSE became apparent.
Issues stemmed, in part, from a fundamental feature of these algorithms: organization of retained strata in contiguous, sorted order.
Forthwith, we refer to algorithms following this convention as ``column''-based.
The column design, and attendant implementation details, impose various drawbacks:
\begin{itemize}
\item \textbf{wasted space}: an annotation size cap can be guaranteed, but a percentage of available space typically goes unused;
\item \textbf{high-level feature dependencies:} in places, existing column code use complex data structures with dynamically allocated memory to perform operations like set subtraction,
\item \textbf{annotation size scaling:} slide-down procedures to maintain sorted order among differentia can take time linearly proportional to annotation size.
% \item \textbf{expensive operations:} in places, slow divide and modulo operations. heavy use
\end{itemize}

\input{fig/surf-vs-column-schematic}

Addressing these concerns required fundamental reformulation of hereditary stratigraphy conventions.
To this end, we introduce a constant-time indexing scheme that maps each lineage's fingerprint stream directly onto a fixed-width memory buffer.
Differentia pruning occurs implicitly, as a result of resident differentia being overwritten by new placements.
As before, care is taken to guarantee temporally-representative collections of resident differentiae within the memory buffer over elapsed time.
We refer to this new approach as ``surface''-based, as opposed to ``column''-based.
Figure \ref{fig:surf-vs=column-schematic} contrasts the two.
% to distinguish it from earlier conventions


% New approach uses a constant-time indexing scheme that maps each lineage's fingerprint stream onto a fixed-width memory buffer such that eviction of existing fingerprint values by new placements maintains a temporally-representative sampling over elapsed time .
% We call this approach ``surface''-based to distinguish it from earlier hereditary stratigraphy methods.
% As before, time stamps can be positionally inferred and thus do not need to be stored --- a several-fold savings that enables fingerprint values to be shrunk down and packed together.


\input{fig/surf-algorithms}

In the course of this work, steady- and tilted-policy surface-based hereditary stratigraphy algorithms have both been developed.
Figure \ref{fig:surf-algorithms} depicts implementation behaviors, with the top panels tracking how placements are sequenced over time in buffer space and the bottom panels showing the consequent distributions of retained time points across history.
We leave formal descriptions of underlying indexing algorithms to future work.
However, reference implementations can be found in supplementary material (Listing TODO).
% For this work, we also translated the tilted algorithm to Zig--like Cerebras Software Language for use on the WSE and, as an intermediate step, to the general-purpose Zig programming language.
% These implementations can be found in associated software repositories with the project.
These algorithms are notable in providing a novel and highly efficient solution the more general problem of curating dynamic temporal cross-samples from a data stream, and may lend themselves to a broad set of applications outside the scope of phylogeny tracking \citep{TODOCITEPREPRINT}.

% async microthread to send from sendBuf to neighbor
% fn dispatchSend_S() void {
%   @fmovs(sendDsd_S, sendBufDsd_S,
%     .{
%       .async = true,
%       .activate = sendFinalizeTaskID_S

% const recvDsd_N = @get_dsd(fabin_dsd, .{
%   .fabric_color = recvColor_N,
%   .extent = recvBufSize,
%   .input_queue = @get_input_queue(q_in_N)
% });
% // open an async microthread to recv from neighbor into recvBuf
% fn dispatchRecv_N() void {
%   @fmovs(recvBufDsd_N, recvDsd_N,
%     .{
%       .async = true,

% color swapping

\subsection{Wafer-Scale Engine Architecture and Programming Model}

The Wafer Scale Engine comprises a networked grid of independently executing compute cores (Processing Elements or PEs).
Each PE contains special message-handling infrastructure, which routes tagged 32-bit packets (``wavelets'') to neighboring PEs and/or to be processed locally, according to a programmed rule set.
Each Processor Element is equipped with 48kb of private on-chip memory, which can be accessed within a single clock cycle.
Communication with neighboring PEs, too, occurs within a single clock cycle \citep{TODO}.
Processor Elements provide faculties for standard arithmetic and flow-control operations, as well as vectorized 32- and 16-bit integer and floating-point operations.

The WSE device is programmed by providing ``kernel'' code, written in the Cerebras Software Language (CSL), to be executed on each PE.
This language's programming model purposefully reflects underlying capabilities and particularities of the WSE architecture.
CSL organizes code within an event-driven framework, with the programmer defining tasks to be triggered in response to wavelet activation signals exchanged between --- and within --- PEs.
Scheduling of active tasks occurs via hardware-level microthreading, which allows for some level of concurrency.
Special faculties are provided for asynchronous send-receive operations that exchange contiguous, or strided, buffer data between PEs.

For further detail, we refer readers to extensive developer documentation made available by Cerebras through their SDK program.
Access can be requested, currently free of charge, via their website.
For this initial work, we evaluated CSL code on a virtualized $3\times3$ PE array emulated with conventional CPU hardware.
(Scale-down was necessary to accommodate significant emulation-layer slowdowns).

\subsection{Asynchronous Island-model Genetic Algorithm}

We apply an island-model genetic algorithm, common in applications of parallel and distributed computing to evolutionary computation, to instantiate a wafer-scale evolutionary process spanning PEs \citep{bennett1999building}.
Under this model, PEs host independent populations that interact through migration (i.e., genome exchange) between neighbors.

Our implementation unfolds according to a generational update cycle, schematized in Figure \ref{fig:async-ga-schematic}.

Migration, depicted as blue-red arrows in Figure \ref{fig:async-ga-schematic}, is handled first.
% fully-asynchronous approach to migration between neighboring PEs.
% Evolutionary processes tend to occur in asynchronous manner with arbitrary factors influencing things, so this is a reasonable relaxation to make.
Each PE maintains independent immigration buffers and emigration buffers dedicated to each cardinal neighbor, depicted by Figure \ref{fig:async-ga-schematic} in solid blue and red, respectively.
On simulation startup, asynchronous receive operations are opened to accept genomes from each neighboring PE into its corresponding immigration buffer.
At startup, additionally, each emigration buffer is populated with genomes copied from the population and an asynchronous send request is opened for each. %to dispatch wavelets containing genome data from the emigration buffer to the neighbor.
Asynchronous operations are registered to on-completion callbacks that set a per-buffer ``send complete'' or ``receive complete'' flag variable.
In this work, we arbitrarily size send buffers to hold one genome and receive buffers to hold four genomes.
The main population buffer was sized to 32 genomes.

Subsequently, the main update loop tests all ``send complete'' and ``receive complete'' flags.
For each immigration flag that is set, buffered genomes are copied into the main population buffer, replacing randomly chosen population members.
Then, the flag is reset a new receive request is initiated.
Likewise, for each emigration flag set, corresponding send buffers are re-populated with randomly sampled genomes from the main population buffer.
Corresponding flags are then reset and new send requests are initiated.
The bottom right corner of Figure \ref{fig:async-ga-schematic} summarizes the interplay between send/receive requests, callbacks, flags and buffers.

The remainder of the main update loop handles evolutionary operations within the scope of the executing PE.
Each genome within the population is evaluated to produce a floating point fitness value.
For this initial work, we use a trivial fitness function that explicitly models additive accumulation of beneficial/deleterious mutations as a floating point value within each genome.
% However, modular implementation ensures evaluation criteria can be chosen appropriate for underlying experimental objectives.

After evaluation, tournament selection is applied. % with, in our experiments, tournament size 5.
Each slot in the next generation is populated with a genome exhibiting maximal fitness among $n=5$ randomly sampled individuals, ties broken randomly.

Finally, a mutational operator is applied across all genomes in the next population.
% As with evaluation criteria, modular implementation allows mutation operations to be defined based on experimental objectives.
Here, we use a simple Gaussian mutation on each genome's stored fitness value, with sign restrictions used to manipulate the character of selection.
To model purifying selection, we applied a strictly negative mutational effect with probabilty 0.3.
To model adaptive selection, we additionally allowed the possibility of a positive mutational effect, with probability 0.003.
At this point, hereditary stratigraphy annotations --- discussed next --- are updated to reflect an elapsed generation.

The process then repeats, with a self-activating wavelet dispatched to execute the next cycle of the main generation loop, until a generation count halting condition is met.

\subsection{Genome Model}

\input{fig/genome-layout}

% Kernel source code implementing described procedures can be viewed at \url{https://hopth.ru/cl}.
% Our implementation is defined modularity with respect to genome size, layout, mutational operator, and fitness evaluation criteria, allowing for direct re-use of produced software for follow-on evolution experiments on WSE hardware.

At implementation level, genomes were handled as fixed-length segments of raw memory.
Figure \ref{fig:genome-layout} shows an example 96-bit genome layout used for validation experiments.
% Figure \ref{fig:validation-example:genomes} details the content and layout of genomes, and provides example genome values yielded from simulation.
The first sixteen bits were used to identify genomes descending from the same founding ancestor.
At the outset of simulation, founding population members were each assigned a randomized tag value, and the value was inherited without mutation over the course of simulation.
The next 80 bits were used for hereditary stratigraph annotation, 16 bits for a generation counter and the remaining 64 as a field for single-bit differentia managed according to a tilted surface policy.

For benchmarking experiments, a 128-bit genome layout was used instead, with the first 32 bits used for a floating point ``fitness'' value and the generation counter upgraded from 16 to 32 bits.
% A key question will be the extent to which intentionally desynchronizing affects performance, stability, and computation quality.

\subsection{Software and Data Availability}

Software, configuration files, and executable notebooks for this work are available via Zenodo at \url{https://github.com/mmore500/hstrat-wafer-scale} TODO.
% TODO add as a submodule to hstrat-wafer-scale:
% CSL software can be found on at \url{https://github.com/mmore500/wse-sketches/tree/v0.1.0} \citep{moreno2024wse}.
% and hsurf too!
Data and supplemental materials are available via the Open Science Framework at \url{https://osf.io/bfm2z/} \citep{foster2017open}.

Hereditary stratigraphy utilities are published in the \texttt{hstrat} Python package \citep{moreno2022hstrat}.
This project uses data formats and tools associated with the ALife Data Standards project \citep{lalejini2019data} and benefited from many pieces of open-source scientific software \citep{sand2014tqdist,2020SciPy-NMeth,harris2020array,reback2020pandas,mckinney-proc-scipy-2010,sukumaran2010dendropy,cock2009biopython,dolson2024phylotrackpy,torchiano2016effsize,waskom2021seaborn,hunter2007matplotlib,moreno2024apc,moreno2023teeplot,torchiano2016effsize,moreno2024pecking,moreno2024joinem,moreno2024hsurf,moreno2024wse}. %TODO check/update me

WSE experiments reported in this work used the CSL compiler and CS-2 hardware emulator bundled with the Cerebras SDK v1.0.0 \citep{selig2022cerebras}, available via request form on the Cerebras website.
SDK utilities can be run from any Linux desktop environment, regardless of access to CS-2 hardware.

% Cerebras currently makes SDK materials available upon request, which can be .
% In preparation for proposed work, we joined the Cerebras SDK program \citep{selig2022cerebras} and assembled Cerebras Software Language (CSL) software implementations that will be necessary to conduct experiments with WSE hardware.
% Access to the SDK can be requested, currently free of charge, via their website.
% For the time being, we have used Cerebras' hardware emulator to test our software implementations.
